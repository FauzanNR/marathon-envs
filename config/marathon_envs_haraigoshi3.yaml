behaviors:
  default:
    trainer_type: ppo
    hyperparameters:
      batch_size: 2048  # Increased for faster training
      beta: 1.0e-4  # Reduced for more exploration
      buffer_size: 20480  # Increased for more training samples
      epsilon: 0.2  # Reduced for faster convergence
      lambd: 0.95
      learning_rate: 5.0e-4  # Increased for faster learning
      num_epoch: 6
    network_settings:
      normalize: false
      num_layers: 2
      hidden_units: 256  # Increased for more model capacity
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        strength: 1.0
        gamma: 0.99
    max_steps: 7.0e4  # Increased for longer training
    time_horizon: 128  # Increased for longer time steps
    threaded: true
    keep_checkpoints: 5
    checkpoint_interval: 200000
    summary_freq: 1000

  # MarathonManBackflip-v0:
  #   trainer_type: ppo
  #   hyperparameters:
  #     num_epoch: 6  # Reduced for faster training
  #     beta: 1.0e-5  # Adjusted for more exploration
  #     epsilon: 0.1  # Reduced for faster convergence
  #     lambd: 0.95
  #     learning_rate: 2.0e-4  # Increased for faster learning
  #     batch_size: 8192  # Increased for faster training
  #     buffer_size: 163840  # Increased for more training samples
  #   time_horizon: 128  # Increased for longer time steps
  #   summary_freq: 50000  # Reduced for more frequent summaries
  #   network_settings:
  #     num_layers: 2  # Increased for more model capacity
  #     normalize: true
  #     hidden_units: 512
  #   reward_signals:
  #     extrinsic:
  #       gamma: 0.99
  #       strength: 1.0
  #       network_settings:
  #         normalize: true
  #         hidden_units: 512
  #         num_layers: 2
  #   max_steps: 128e6  # Adjusted based on the expected training time
  #   threaded: true
  MarathonManHaraigoshi:
    trainer_type: ppo
    hyperparameters:
      num_epoch: 6  # Reduced for faster training
      beta: 1.0e-5  # Adjusted for more exploration
      epsilon: 0.2  # Reduced for faster convergence
      lambd: 0.95
      learning_rate: 2.0e-4  # Increased for faster learning
      batch_size: 8192  # Increased for faster training
      buffer_size: 163840  # Increased for more training samples
    time_horizon: 128  # Increased for longer time steps
    summary_freq: 50000  # Reduced for more frequent summaries
    network_settings:
      num_layers: 2  # Increased for more model capacity
      normalize: true
      hidden_units: 512
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 512
          num_layers: 2
    max_steps: 128e6  # Adjusted based on the expected training time
    threaded: true

  MarathonManHaraigoshi-v2:
    trainer_type: ppo
    hyperparameters:
      num_epoch: 16
      beta: .0005
      epsilon: 0.2
      lambd: 0.95
      learning_rate: 1e-4
      batch_size: 4096
      buffer_size: 131072 # 4096 * 32
    time_horizon: 256
    summary_freq: 100000
    network_settings:
      num_layers: 2
      hidden_units: 512
      normalize: true
    reward_signals:
      extrinsic:
        gamma: 0.95
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 512
          num_layers: 2
    max_steps: 198e6
    threaded: true
  
  MarathonManHaraigoshi-v3:
    trainer_type: ppo
    hyperparameters:
      num_epoch: 16
      beta: 0.0005
      epsilon: 0.2
      lambd: 0.95
      learning_rate: 5e-5
      batch_size: 4096
      buffer_size: 65536 # 4096 * 16
      # entropy_coef: 0.01
      # max_grad_norm: 0.5
    network_settings:
      num_layers: 2
      hidden_units: 256
      normalize: true
    reward_signals:
      extrinsic:
        gamma: 0.95
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 128
          num_layers: 2
    keep_checkpoints: 5
    max_steps: 198e6
    time_horizon: 256
    summary_freq: 50000
    checkpoint_interval: 200000
    threaded: true
  MarathonManHaraigoshi-v5:
    trainer_type: ppo
    hyperparameters:
      num_epoch: 8
      beta: 0.0008
      epsilon: 0.2
      lambd: 0.95
      learning_rate: 5e-5
      batch_size: 4096
      buffer_size: 16384 # 4096 * 4
      # entropy_coef: 0.01
      # max_grad_norm: 0.5
    network_settings:
      num_layers: 3
      hidden_units: 256
      normalize: true
    reward_signals:
      extrinsic:
        gamma: 0.995
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 256
          num_layers: 3
    keep_checkpoints: 4
    max_steps: 198e6
    time_horizon: 1000
    summary_freq: 50000
    checkpoint_interval: 200000
    threaded: true
